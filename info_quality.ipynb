{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkCT6BZfbp6NoPAoRx/8tY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detsutut/infoquality-dca/blob/master/info_quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyphen"
      ],
      "metadata": {
        "id": "63lMNNl_Jji2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnA-NfMNJNOm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import pyphen\n",
        "import os\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "dic_ita = pyphen.Pyphen(lang='it_IT')\n",
        "dic_eng = pyphen.Pyphen(lang='en_EN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentences(text: str):\n",
        "    text = remove_URL(text)\n",
        "    text = text.replace(\"\\n\\n\",\". \")\n",
        "    text = text.replace(\"\\n\",\". \")\n",
        "    sents = nltk.tokenize.sent_tokenize(text)\n",
        "    return sents \n",
        "\n",
        "def remove_punct(text: str):\n",
        "    return re.sub('\\W+\\s*', ' ', text)\n",
        "\n",
        "def remove_URL(sample):\n",
        "    \"\"\"Remove URLs from a sample string\"\"\"\n",
        "    return re.sub(r\"http\\S+\", \"\", sample)\n",
        "\n",
        "def clean_text(text: str):\n",
        "    text = remove_URL(text)\n",
        "    text = text.replace(\"\\d\", \"\")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = remove_punct(text)\n",
        "    text = text.replace(\"[1-9 ]{1,}\", \" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_syllab(word: str, dic: pyphen.Pyphen):\n",
        "    return dic.inserted(word).split(\"-\")\n",
        "\n",
        "def get_long_words(text:str):\n",
        "    return [word for word in text.split() if len(word)>6]\n",
        "\n",
        "def get_RIX(text: str):\n",
        "    sentences = split_sentences(text)\n",
        "    long_words = get_long_words(clean_text(text))\n",
        "    return len(long_words)/len(sentences)\n",
        "\n",
        "def get_syllab_score(text: str, dic: pyphen.Pyphen):\n",
        "    words = text.split()\n",
        "    syll_w = [len(split_syllab(word, dic)) for word in words]\n",
        "    syll_s = sum(syll_w)\n",
        "    return syll_s / len(words)\n",
        "\n",
        "def get_gunning_fog(text: str, dic: pyphen.Pyphen):\n",
        "    text_clean = clean_text(text)\n",
        "    words = text_clean.split()\n",
        "    sentences = split_sentences(text)\n",
        "    complex_words = [word for word in words if len(split_syllab(word,dic))>=3]\n",
        "    W = len(words)\n",
        "    S = len(sentences)\n",
        "    D = len(complex_words)\n",
        "    return round(0.4*((W/S)+100*(D/W)),1)\n",
        "\n",
        "def get_gulpease(text: str, dic: pyphen.Pyphen):\n",
        "    text_clean = clean_text(text)\n",
        "    words = text_clean.split()\n",
        "    sentences = split_sentences(text)\n",
        "    LP = len(text_clean)*100/len(words)\n",
        "    FR = len(sentences)*100/len(words)\n",
        "    return round(89-(LP/10)+(FR*3),1)\n",
        "\n",
        "def get_flesch(text: str, dic: pyphen.Pyphen, type=\"ENG\"):\n",
        "    text_clean = clean_text(text)\n",
        "    words = text_clean.split()\n",
        "    sentences = split_sentences(text)\n",
        "    total_num_words = len(words)\n",
        "    total_num_sentences = len(sentences)\n",
        "    W = total_num_words/total_num_sentences # numero medio di parole per frase\n",
        "    S = get_syllab_score(text_clean, dic) \n",
        "    if type==\"ITA72\":\n",
        "      index = 206-(0.65*W)-(100*S)\n",
        "    elif type==\"ITA86\":\n",
        "      index = 217-(1.3*W)-(61.8*S)\n",
        "    else:\n",
        "      index = 206.835-(84.6*S)-(1.015*W)\n",
        "    return {\"words\":total_num_words, \"sentences\": total_num_sentences, \"W%\": W,\"S%\":S,\"index\": round(index,1)}\n"
      ],
      "metadata": {
        "id": "GbgO3JqiJ7Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "FuBJ0SepFgBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google\n",
        "\n"
      ],
      "metadata": {
        "id": "kDd0vmY0OrsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/gdrive/MyDrive/Your/Path/Here/Corpus/10Googles\"\n",
        "dirs = [s.name for s in os.scandir(path) if s.is_dir()]\n",
        "for dir in dirs:\n",
        "  lan = \"en\" #ita en\n",
        "  path_of_the_directory = f\"{path}/{dir}/{lan}/\"\n",
        "  rows = []\n",
        "  for filename in os.listdir(path_of_the_directory):\n",
        "      f = os.path.join(path_of_the_directory,filename)\n",
        "      if os.path.isfile(f):\n",
        "          f = open(f, 'r')\n",
        "          text = f.read()\n",
        "          fv = get_flesch(text, dic_eng, type=\"ENG\") #remember to change dic as well when changing language\n",
        "          rows.append({'name':filename,\n",
        "                      'words':fv['words'],\n",
        "                      'sentences':fv['sentences'],\n",
        "                      'words per sent':round(fv['W%'],2),\n",
        "                      'syllabs per word':round(fv['S%'],2),\n",
        "                      'RIX': round(get_RIX(text),2),\n",
        "                      'flesch':fv['index'],\n",
        "                      'flesch_vacca_72':fv['index'],\n",
        "                      'flesch_vacca_86':get_flesch(text, dic_ita, type=\"ITA86\")['index'],\n",
        "                      })\n",
        "          f.close()\n",
        "  df = pd.DataFrame(rows)\n",
        "  df.to_csv(path_or_buf=f\"{path}/{dir}_index_{lan}.csv\",sep=\";\")"
      ],
      "metadata": {
        "id": "85zU5_M9J70g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wiki"
      ],
      "metadata": {
        "id": "nugGgDznOoTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/gdrive/MyDrive/Your/Path/Here/Corpus/Wikipedias\"\n",
        "lan = \"en\" #ita en\n",
        "path_of_the_directory = f\"{path}/{lan}/\"\n",
        "rows = []\n",
        "for filename in os.listdir(path_of_the_directory):\n",
        "    f = os.path.join(path_of_the_directory,filename)\n",
        "    if os.path.isfile(f):\n",
        "        f = open(f, 'r')\n",
        "        text = f.read()\n",
        "        fv = get_flesch(text, dic_eng, type=\"ENG\") #remember to change dic as well when changing language\n",
        "        rows.append({'name':filename,\n",
        "                    'words':fv['words'],\n",
        "                    'sentences':fv['sentences'],\n",
        "                    'words per sent':round(fv['W%'],2),\n",
        "                    'syllabs per word':round(fv['S%'],2),\n",
        "                    'RIX': round(get_RIX(text),2),\n",
        "                    'flesch':fv['index'],\n",
        "                    'flesch_vacca_72':fv['index'],\n",
        "                    'flesch_vacca_86':get_flesch(text, dic_ita, type=\"ITA86\")['index'],\n",
        "                    })\n",
        "        f.close()\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(path_or_buf=f\"{path}/index_{lan}.csv\",sep=\";\")"
      ],
      "metadata": {
        "id": "OXxJr9rHMXAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}